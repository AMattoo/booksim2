\documentclass{article}
\usepackage{amsmath}
\usepackage{epsfig}
\usepackage[all]{xy}
\usepackage{multicol}
\usepackage{indentfirst}
\usepackage{algorithm}
\usepackage{algorithmic}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9.5in}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{0in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\topskip}{0.5in}
\begin{document}
%% be sure to change the:
% Section number, Day, and time
% Date and year
% Name
% spellcheck using ``aspell -c filename'' in Unix
\begin{center}
\LARGE{\bf Parallelizing a Network Simulator}
%% PUT IN YOUR SECTION, DAY, AND TIME
%%example:
%% Section 7 - Thursday 8:30 AM\\

%% PUT IN THE DATE AND YEAR THE ASSIGNMENT IS DUE
%%example:
%% September 10, 2004
%% PUT IN YOUR NAME
\begin{center}
\Large{
Michael Bauer \hspace{1in} Nan Jiang
}
\end{center}
\Large{Department of Computer Science \\ Stanford University \\ \{mebauer, njiang37 \}@stanford.edu}
\end{center}

\begin{multicols}{2}
\begin{center}
\Large{\bf Abstract}
\end{center}
%\small{
\emph{In the context of the current multicore revolution, the need to
design an effective interconnect network has become increasingly
important.  Computer architects make use of network simulators in order to
determine the best designs and configurations.  The process by which this
is done however is often computationally intensive and requires
significant cycle time.  The fact that most simulators are single threaded
only adds to the computational overhead and renders them unable to take
advantage of new multicore processors.  This paper demonstrates how
an interconnect network simulator can be parallelized into a multithreaded
implementation.  The additional
performance gain from this parallel simulator ultimately can lead to decreased
development time and better designs as architects can sweep additional
parameter spaces and consider more configurations.}
%}
\section{Introduction}
After hitting the frequency and power walls, chip manufacturers have begun
to design and produce multicore chips that are capable of executing
multithreaded programs.  While this has the potential to yield many orders
of magnitude in performance gains, existing programs that have been
written in a single threaded programming model must be rewritten in a
parallel programming model in order to take advantage of multicore
processors.  This process of converting a program from a single threaded
programming model to a parallel programming model is a non-trivial task
that often requires restructuring the algorithm of the program as well as
how the data is organized and partioned.  In this paper we demonstate
several parallelization techniques in the process of parallelizing a
network simulator.\\
~\\
Another major ramification of the multicore revolution has been the
increased importance of interconnect network design.  As the number of
cores being placed on a chip increases, the network plays a bigger and
bigger role in minimizing the latency of communication between processors
and providing enough bandwidth to allow for the movement of large chunks
of data.  In order to make informed decisions in the process of designing
interconnect networks, computer architects make use of simulators to
observe the effect of different configurations and parameters.  While
these simulations can provide valuable insight into the best designs, they
often require sweeping a large, multi-dimensional parameter space.  In
addition to this, each simulation requires that many thousands of cycles
of operation be simulated.  This demands a large number of computational
resources in order to fully explore the design space.\\
~\\
The network simulator that we employ in this papaer was originally written
in a single threaded programming model.  This made the simulator extremely
slow and a single simulation of just a few thousand cycles would take many
minutes of compute time.  In order to speed up the process of running
simulations we have parallelized this simulator by making it
multithreaded.  In section \ref{background} we provide some background on
the simulator and its mechanics.  In section \ref{para} we describe the
means by which we parallelized the simulator.  We then describe in section
\ref{results} the results that we obtained from our parallel
implementation.  After demonstrating our performance gain and its limiting
factors we discuss how additional performance could be gained in section
\ref{disc}.  Finally, in section \ref{conc} we conclude.

\section{Simulator Background \label{background}}

\section{Parallel Implementation \label{para}}
In order to parallelize the network simulator, we first set about
identifying sources of parallelism that we could exploit.  Unlike a
mirco-architectural simulator, the network simulator doesn't have many
functional dependencies.  Each router in the network can be simulated
independently as long as the communication that occurs between the routers
is synchronized in the channels.  In this section we describe how we
partitioned the work of the network simulator and the synchronization
mechanisms that we employed in order to maintain the correctness of the simulator.

\subsection{Partitioning the Network}
The first step in parallelizing the simulator was to define a division of
work that could be spread across parallel threads with minimal amounts of
communication overhead.  Fortunately, the network simulator naturally
lends itself to this sort of partition in the sense that each router
within the network can be simulated almost entirely independent of every
other router.  In addition to this communication only occurs at the
beginning and end of every cycle of simulation when flits are either
placed into or pulled out of the shared channels.  The significant
calculation of the arbitration logic is a much larger computational that
can be performed in an entirely local manner.  While this provided us with
a simple partition of work, it left us with several important issues both
for correctness and performance.

\subsubsection{Locking Queues}
We initially assigned routers to threads by their ID numbers which are completely indpendent of
topology.  This left us no way of determining which channels would
ultimately be shared and which would be local to a thread, therefore we
were forced to place locks on all channels to ensure correctnes.  Since
the channels were intially represented as STL queues, there was no way to
allow concurrent accesses to the queues (i.e. we couldn't allow one router
to be reading and another to be writing simultaneously).  In order to
prevent this, we protected each queue with a Pthread lock.  For a thread
to access the queue, it first had to acquire the lock and then release the
lock when finished performing its operation on the queue.\\
~\\
While this approach guaranteed correctness, it also caused significant
performance overhead.  Even queues that were not shared between different
threads were having to be locked and unlocked in order to perform
operations on the queues.  In order to get around this additional
overhead, we instead began to partition the router work based upon the
topology of the network.  This therefore allowed us to know which channels
were shared and which were local to a single thread.  By doing this, we
were able to remove some of the overhead of locking when performing
operations on a channel's queue.

\subsubsection{Barriering Every Cycle}
While the difficulty of efficiently partitioning the network created
several performance issues, we also had to address the issue of the
correctness of the simulation.  By allowing each thread to operate
independently, we had created the potential for one or more threads to run
ahead of other threads in the simulation.  This means that different
threads could be performing computation on different cycles of the
simulation at the same time.  If not handled correctly, this could have
led to a correctness issue.  Consider the following example: assume that
one thread is simulating one cycle ahead of all of the other threads.  In
order for this thread to execute, it first reads in all the flits that are
in its input channels.  However, all the other threads have yet to write
to their output channels.  Therefore when the thread running ahead
performs its reads, it will observe that all its input channels are empty
and assume this is correct, when in reality, the flits simply have not
been written from the previous simulation cycle.\\
~\\
In order to successfully solve this data race, we initially place a
barrier after every cycle of the simulation.  This then mandated that
after a thread simulates a cycle, it must wait for all the other threads
in the simulator to reach the same state.  This ensures that all flits are
written into their proper channels before any thread attempts to read them
out when simulating the next cycle.  While this guarantees correctness, it
does not encourage much parallelism among the threads and adds
synchronization overhead.  In the next section we describe how we were
able to remove the barrier and the locks around the queues in order to
improve concurrency and minimize synchronization overheads.

\subsection{Lock-Free Queues}
In order to improve the concurrency of our network simulator, we decided
to incorporate the lock-free queues described in \cite{LF}.  The advantage
that these queues provide is that they allow multiple readers and writers
to the queue simulataneously.  While our simulator only requires that we
be able to have one reader and one writer, simultaneously, these queues
still allow us to remove any lock overhead from the simulation.  For
completeness we give a brief overview the implementation of lock-free
queues.

\subsubsection{Implementation}
Lock-free queues are based on the universal concurrent access algorithm
\cite{LF} that assumes that any access to the shared data structure can
see the data structure in any potential partially updated state.  The
algorithm then allows for the access to either help other accesses to
complete before actually completing itself.  While this may sound
conceptually simple, the combinatorial number of states of the data
structure and different readers and writers is astounding.\\
~\\
The lock-free queue implementation that we use begins by having a sentinel
at the head of the queue whose value is meaningless.  This eases some of
the implementation details of the queue.  Initially both the head and tail
pointers of the queue point to the sentinel.  Whenever a new item is added
to the queue, the tail pointer is moved to point to the end of the list.
Whenever an item is popped from the queue, the head is moved to point to
the new sentinel, the next item in the list.  In this case the old
sentinel can then be reclaimed.  Each element in the lock-free queue is
rapped in an AtomicReference object.  This object ensures that the any
items in the queue can only be written to atomically.  A diagram of how
this works can be seen in Figure (need figure here).\\
~\\
We initially implemented this AtomicReference object with a Pthread
lock.  While this provided a finer granularity of access to shared
objects than previously, we realized that the overhead in the number of
locks was tremendous.  Instead we relied upon an extension to GCC 4.1.2
(need citation) that provided an atomic \_\_compare\_and\_exchange operation.
This operation was significanlty less costly in terms of synchronization
overhead and was also much faster.\\
~\\
After testing our lock-free queues, we then went about replacing all of
the queues in the simulator with our new queue implementation.  While this
proved fairly straight forward, we didn't initally account for the high
overhead that the lock-free queues themselves created.  The lock-free
queues involve the creation of multiple objects and accessing the
lock-free queues involves chasing many pointers through these structures
as well as rearranging many pointers.  In some cases this overhead proved
to be extreme.  In order to remove it, we created a common interface that
was shared between the lock-free queues and the STL queue.  This then
allowed us to create dynamic instances of either a shared queue or an STL
queue depending on whether the channel was shared.  This significantly
improved our performance as we will demonstrate in the section \ref{results}.

\subsubsection{Guaranteeing Correctness Without a Barrier}
After adding the lock-free queues to our implementation, we realized that
they would provide little benefit if the barrier were not also removed to
allow threads to run ahead.  In order to remove the barrier, we had to add
an additional piece of data to each shared queue: a semaphore.  This
semaphore would be used in the following way: anytime a write operation
occurred to the queue, an up operation was performed.  Similarly, anytime
a pop operation occurred to the queue, a down operation would have to be
performed first.  We also modified the routers such that for every cycle
of simulation, they would at perform at least one up operation on every
channel that they shared with another thread even if they didn't actually
write any flits to that channel.  Note that a down operation is not able
to proceed if the semaphore equals zero, implying that no router can ever
run ahead of any of its input queues.  By adding this feature to our
lock-free queues we were able to guarantee correctness in the simulator
without a barrier operation.  This greatly increased the amount of
concurrency present in the simulation by allowing different threads to be
simulating different cycles simultaneously.

\subsection{Fusing Loops}
The last operation that we performed on our code was to fuse several loops
together.  The initial simulator implementation was written to make it
easy to understand the code and not for performance.  In some cases,
multiple independent 'for' loops were used to perform different
operations.  In several cases we moved all these operations into a single
'for' loop in order to help improve some of the temporal locality of the
code.  Not surprisingly this operation allowed the system to take
advantage of the cache hierarchy and provide additional perfromance gain.

\section{Results \label{results}}

\section{Discussion \label{disc}}

\section{Conclusion \label{conc}}


\begin{thebibliography}{99}
\bibitem{LF} Herlihy and Shavit {\em The Art of Multiprocessor Programming.} 2008.

\end{thebibliography}
\end{multicols}

\end{document}
